lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
best10_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056127, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.568653427817313, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ents=[(17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']), (17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']), (17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']), (17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']), (17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']), (17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']), (17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']), (17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']), (17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']), (17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要'])]
answer_open_question_3="We can see the tweets at the top of the list are in English,at an entropy\naround 2.5,and the tweets at the end of the list are in Japanese,at around 17.5.This is a big range,going from a\ngood entropy of 2.5,compared to the human 1.3,to 7 times that.This makes sense, as our model was trained in English\non the Brown corpus,not Japanese,so it has learned bigrams of letters in English,and can predict English\nwords,whereas it has never seen Japanese characters so can't predict what comes next"
answer_open_question_4='Still contain emoji parts,eg :D ->d.They will only be 1 char,so we can remove all\nwords<1,but a i and u\nHas chars not in the Latin alphabet,eg Japanese.Remove words not containing ascii chars 97-122\nHas words in foreign languages,eg Spanish.Use language detection eg langdetect,to identify and remove non-English\nwords\nHas misspellings,use another package eg TextBlob to correct\nHas shortenings/slang,eg lol.Can create helper functions to expand and standardize slang u->you,ty->thankyou'
mean=3.8435755769050926
std=0.47772976561662
best10_ascci_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056127, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.568653427817313, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ascci_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.166728812980961, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
best10_non_eng_ents=[(4.321320405509358, ['afganistán', 'asociación', 'de', 'de', 'mujeres', 'rawa', 'revolucionarias']), (4.321322108677053, ['carrey', 'face', 'feat', 'mariah', 'minaj', 'my', 'nicki', 'out', 'rt', 'up', 'video', 'xxlmag', 'com']), (4.321338322311484, ['abisss', 'aja', 'demo', 'gini', 'hari', 'hikmah', 'mantabsss', 'membawa', 'sepi', 'sudirman', 'thamrin', 'tiap', 'trnyta']), (4.321374586541905, ['a', 'a', 'agora', 'com', 'consegui', 'd', 'de', 'dormir', 'dormir', 'durmo', 'e', 'eu', 'inteira', 'mas', 'nao', 'nao', 'nao', 'nao', 'noite', 'noite', 'nove', 'q', 'se', 'sono', 'to', 'vou']), (4.321385367081537, ['eh', 'meu', 'o', 'que', 'twitter', 'esse']), (4.321390995790845, ['don', 't', 'get', 'giggle', 'i', 'i', 'oh', 'oh']), (4.321500139376771, ['am', 'geburtstag', 'gefeiert', 'klingende', 'november', 'töne', 'wird']), (4.321525080235015, ['attraktion', 'belønning', 'fest', 'kærlighed', 'lykke', 'privacy', 'succes', 'tarot', 'tillid', 'udholdenhed']), (4.321575437028816, ['cerca', 'de', 'de', 'deci', 'del', 'exameeen', 'examen', 'le', 'mateeee', 'matematica', 'pueda', 'que', 'rt', 'shhuu', 'shuuu', 'sientese', 'valla', 'wayyy', 'y']), (4.321622845063304, ['abre', 'china', 'huaehuiaieh', 'o', 'olho'])]
worst10_non_eng_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.166728812980961, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
answer_open_question_6='1:It says English in general.We cant get a dataset of the whole English\nlanguage,but to simplify we can pick a dataset to represent it and exemplify its different forms\nspeaking/written,formal/informal,modern/historical eg the BNC\n\n2:It doesnt specify what probability is used to calculate entropy,humans can be used to form a prediction,or we can\nuse a model to simplify eg ngrams\n\n3:It says entropy,but we want the entropy of our models prediction relative to the true probability, this is cross\nentropy.We cannot know the true prob,as again it requires all English,so to simplify we use an estimation of cross\nentropy using what occurs in our corpus\n\nThe question is then what is the estimated cross entropy of an ngram model on the BNC.To carry this out,I would\nclean the data of punctuation and lowercase it,then train an ngram model and measure the cross entropy.As the BNC is\nlarge it enables us to split the data into multiple rounds of training and testing to get the strongest value for n'
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[0.006913064743369809, 0.0012190937826217086, 0.12333945519178972, 2.2315401420598457e-06, 2.6766530157362866e-05, 0.004917741586184577, 0.004933935254094318]
naive_bayes_posterior=[{'V': 0.5886037204341108, 'N': 0.41139627956588926}, {'V': 0.15633267093794534, 'N': 0.8436673290620547}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.9961437486715612, 'N': 0.003856251328438816}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7949987620698192
answer_open_question_8='We can see that using all the words as features gives the best accuracy.Whereas just one\nword,gives a lower accuracy.This makes sense,as when naturally assigning attachment,we do use all the words in a\nsentence,as they can all have impact.But of those,P is the most informative,as it is only 5% less accurate than\nusing all 4\n\nWhen comparing LR with NB,NB has a little lower accuracy.This is most likely because NB cannot weight the most\ninformative features,but takes them all into account equally'
lr_predictions='VVVVVVVVVNVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVNNNVNNNVNNVVVNNVVVNVNVVNVNVNNNNNVVVNVNNNVNVNVNNVNVVVVNNNNVNNNNVVVNVVNVNNVNNVNNVNVNNNVVNNNNVNNNNVNNNNNVNNNNVNNNNNNNNNNVNNNVVVVVVVNVNVNNNNVVVVNVVVVVVNNVNNVVNVNVNNNNVNVVVNVNVNNNNVNVNVNNVNNVNNVNNVNVVNNNNVNVNVNNVNVVNNNNVVNVNVNVVNVVVNVVNVNNVNNVVNNNVNNVVNNNNNVNVNNVVVNNNVVVVNVVVVVNNNNNNVNVNNNNNVNVNVNNVVNVNNNNNVNVNVVNVVVNNNVVVNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVNVVVVVVNNNVNVNVVNNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNNNNVVNVNNNVNVNNNVVNVNNVNNNVNVVVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNVNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVNVVVVNVNNNVVNNVVVNNNNVNVVNVVNVNNVVNVVVVVVVVNVNVVVVNNVNVVNVVNNVNNNNVVNNVNVNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVNVNNNNNVVNNVNVNNNNVVNVNVVVNNVNVVNVVVNVNNNNNNNVNNVNVNNVVVVNNNVNNVVVNVVVVVVVVNVNVVVNVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVNVVVNVVVVVVNVVNVNNNNVNNNVNNNVNNNNVVNNVNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNVVNVVVVNVNVNNNVVVNNNVNNVNNNNNNNVVVVVVNNVVVVVNNVVVVVNNVVVNNNVVVNNVNNVNNNNNNNNVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNNNVNVVVVVNVVNVNNVVVNNNNNVNNNNVNVNNVNVNNNNVVNNNVNNNNNNVVVVVNVNNNVNVNNNVVVVVVVVVNVVVNVNNVNVVVVNNVVNVVNVVVNVVVVVNNNNNNNNNNNNNNVNNVNNNVNNVNNVVVVNVVNVNNNVVVNVVVNNVNNNVVVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNVVNVVVNVNVNNNNVVNNNNNVVVNVNNNNNNVVVVNNVNVNVNVNNVVVNVNNVVVNVVNVVNVNNVNNNVNVVNNVVVNVVVVVVVVNVVNNNNVNVVNNNVVNNVNNVNVNNVVVNVNNNVVVNNVVVVNNNNVNVNNVVVVVNNNNNNVVVVNVNVVNVVNVNNNNVVNVVVNNNNVVNNVNNNNNVNNNNNNVNNNNNNNNVNNNNVVVNVVNNVNNVVVNNNNNVNNVVNNVVVVNNVNNNVVVNNNNVNNVNVNNNVNNVNNVNNVNNVNNVNNVNNNNVVNVNVVNNVNNNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVNVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVVNVNNNVNVNVNNVVNVVNNVNVVNNVVVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVNVVNVVNVNVVVNNVVNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNNNVVVNVVVNNNNNVNVNNNVNVNNVVNNNNNVNNNNNVNNVNVVNNNVVNNNVVVNVNVNVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNVVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNVNNNNVNNVVNNVNNNNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNVNVNNNVVNVNVNVNNVVNNNVVNNVNNVNNNNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVVVVNVVVNNVNVNNVVNNNVVNVVNNVNVVVNVVVNVVVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNNNVNVVVNVVNVNVVNNNVNNVNVNVVNNVVVNNVVVVVNNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNVVNVNVNVNVNNNVNNVVVNNNNVNNVVNNVVNVVNVVNNVNVNVVNVVVNNNVNVVVNVNNVVVVNNNNVVVVVNVVNVVVNVNVVVNVVNNVVVNNNVNNNNVNNNNVVNVNNVVVNNVNVNVVNVVNVNVNNVVVVNNVNVNVVVVVNVNNVVNVNNNNNVVNNVNNNVNNNVNVNVVVNVVVNNVVNNNNNVVVNNNVNVNNVVNVVNNVNVNNNNNVVVVNNVNVNNVNNVVVVVNVNVVVVVVVNNNVNNNVNNVNVVNVNVNVNNVVVNNNVNNNNVNNVVNNVNVNNNNVVVNVVNVNVNNVVNVNVVNVNVVVVNNVNVVVNNVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNVVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVVNNVNNVNNNNNNVVNNVVVNNNNVVVNVNNVNNNNNVVVNVVNNVNVNNVVVVVVNVNVVVVVVVVNVNNNNNVVNVNVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNNVVVNNVNVVVVNVNNVNNVVNNVNVVNVNNVNNNVNNVVVVVNVVVNNVVVVNVNNVNNNVVVNVVVVVVVVNNVNVNVVNNVVVVVVVVNNVVVNVVVNVVVVNVVVNNVNVNVVVVNVVVVNNNVVVNVVVNNNVVNNVNNVNVNVNNVNVNNNNNVNVNVVVVNNNNNNNNNNNVNNVNVNNNNNVNVNNNNNVNVVVNVNNNVVVNVNVVVVVNNNNNNNVVVVVVNVVVVVNVVVVVNNVVNNVVNVVVNNNNNNNVVNVNNVVVNNNNVVNNVNNVVVVVVVNVVVNVNVVVNVNVVNVNVNNNVNVNNNNNNVVVNVNVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVNVVVVVNNNVNNVNNVVNVNNVVVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNVNVVVVVNNVNVNVNNNVNNNNNVVVVVVVNVNVVVVNVVNNVNVNNNNNVNVVVVNVNNNVVVNNVVNNNNNNVVVNVNVVNNNVVNNNVVVVNVVNVVVNNVVNVVNVNVNNVVNNNVNNVNNVVVVNNVNVNNNVNVNVNVVVVVNVNVNNVNNVNNVVVVNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNNNNVVNNNVNVVNNNVVVNVNNNNVNNNVVVVVVVVVVNVVVVNNNNVVVNVVNNNVNVNNNNNNNVVNVNNNNVVVNVVNNNNVVVNVVNNNVNNVVVVNNVNNVNVVNVNNNNNVNVVNNNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVVVVNNVNVVVVNNVVNVVNVVNNNNNVNVVNVNVVVNVNNNVVVVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVNNNVNNNNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVNVNNVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNNVNNNNNVNNVVNVVVNNNVVNNVNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNNVNVNNNVVNNNNNNNVNVNNNVNVNVNVNVNVNNVNNNVNVNVNVVVNNNVNVNNNNNVNNNNNNVNVVVVNNVNVNNNNNNVNNVNNNVNNNVVVVVVNVVNNNVVNVVVNNVVNVNVNVNVVNVVNNVNVVVVVVVNNNNNNNNVNVNNNVNNNNNVNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVNNVVVNVVNNVVVNVVNNVVVVNNVNVNVNVNNVVNVNNVVVVVVNVVNNVNVNNVN'
answer_open_question_9='Clearly from q8 using single words in the PP is\ninformative.My template attempts to extend this,including combinations of these words as well.Eg the combination of\nthe N1 and the P,as these are what we use to naturally tell attachment.I also tried to use the information from the\nnouns that cannot be gained from the words alone,eg POS\n\n1.(v=rose,p=to),(v=fell,p=to) I started using just (v,p),and began to see these patterns with a verb and "to"\nindicating noun attachment.This is because the P "to" alone occurs too frequently and does not indicate\nattachment,but in conjunction with some verbs like rose,it does\n\n2.(p=for,n2=years),(p=over,n2=years) This is a similar pattern,but with (p,n2) indicating noun attachment.Alone\nn2=years doesn\'t,but with "for" or "over",it captures our natural understanding of it\n\n3.Low values of len(n1+n2) indicate verb attachment.Not in top 30,but increases accuracy.My only theory is that if\nn1 and n2 are both numbers,so shorter,it is more likely to be a verb'
